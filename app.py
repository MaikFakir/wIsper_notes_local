# --- 1. INSTALACIONES E IMPORTACIONES ---
# Este script asume que las siguientes librer√≠as est√°n instaladas.
# Se pueden instalar con: pip install faster-whisper gradio resemblyzer librosa ffmpeg-python scikit-learn torch numpy

from faster_whisper import WhisperModel
import gradio as gr
from resemblyzer import VoiceEncoder
import numpy as np
import librosa
from sklearn.cluster import DBSCAN
import torch

# --- 2. CARGA DE MODELOS ---

# Determina el dispositivo a usar
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if torch.cuda.is_available() else "int8"

# Cargar modelo Whisper. 'base' es un buen equilibrio entre velocidad y precisi√≥n para timestamps.
# 'turbo' no es un tama√±o de modelo est√°ndar, lo cambiamos por 'base'.
print("Cargando modelo de transcripci√≥n...")
transcription_model = WhisperModel("base", device=DEVICE, compute_type=COMPUTE_TYPE)
print("Modelo de transcripci√≥n cargado.")

# Cargar encoder de voces. Es recomendable usar CPU para evitar conflictos de VRAM.
print("Cargando modelo de codificaci√≥n de voz...")
voice_encoder = VoiceEncoder(device="cpu")
print("Modelo de codificaci√≥n de voz cargado.")


# --- 3. FUNCI√ìN PRINCIPAL DE PROCESAMIENTO ---

def transcribir_con_diarizacion(audio_path):
    """
    Transcribe un archivo de audio y realiza diarizaci√≥n de hablantes.
    """
    if audio_path is None:
        return "No se recibi√≥ audio. Por favor, graba algo."

    # 1Ô∏è‚É£ Cargar audio UNA SOLA VEZ
    # Optimizacion: Cargamos el audio con librosa y lo pasamos como un array de numpy a Whisper.
    # Esto evita que el archivo se lea del disco dos veces.
    try:
        wav, sr = librosa.load(audio_path, sr=16000)
    except Exception as e:
        return f"Error al cargar el archivo de audio: {e}"

    # 2Ô∏è‚É£ Transcripci√≥n con Whisper
    # Pasamos el array de numpy 'wav' directamente al modelo.
    segments_generator, info = transcription_model.transcribe(wav, language="es", word_timestamps=True)

    # Convertimos el generador a una lista para poder acceder a su longitud y elementos m√∫ltiples veces.
    segments = list(segments_generator)

    if not segments:
        return "No se pudo transcribir el audio (posiblemente silencio)."

    # 3Ô∏è‚É£ Extraer embeddings de voz de cada segmento
    embeddings = []
    valid_segments = []
    for seg in segments:
        start_sample = int(seg.start * sr)
        end_sample = int(seg.end * sr)
        segment_wav = wav[start_sample:end_sample]

        # Nos aseguramos de que el segmento de audio no sea demasiado corto para el encoder.
        if len(segment_wav) < 400: # El encoder necesita al menos 400 muestras (25ms)
            continue

        try:
            emb = voice_encoder.embed_utterance(segment_wav)
            embeddings.append(emb)
            valid_segments.append(seg)
        except Exception as e:
            print(f"No se pudo procesar el segmento de {seg.start} a {seg.end}: {e}")


    if not valid_segments:
        # Si no se pudo obtener ning√∫n embedding, devolvemos la transcripci√≥n simple
        return " ".join([s.text for s in segments])

    # Convertimos a numpy array para el clustering
    embeddings = np.array(embeddings)

    # 4Ô∏è‚É£ Clustering de hablantes con DBSCAN
    # El valor 'eps' es el m√°s importante a ajustar. Un valor m√°s bajo crea m√°s clusters (m√°s hablantes).
    # Un valor m√°s alto agrupa m√°s voces juntas. 0.5 es un buen punto de partida.
    clustering = DBSCAN(eps=0.5, min_samples=1, metric="cosine").fit(embeddings)
    labels = clustering.labels_

    # 5Ô∏è‚É£ Construir la transcripci√≥n final formateada
    transcripcion = ""
    current_speaker_label = -1
    current_text = ""

    for i, seg in enumerate(valid_segments):
        speaker_label = labels[i]

        if speaker_label != current_speaker_label and current_speaker_label != -1:
            speaker_name = f"Hablante {current_speaker_label + 1}"
            transcripcion += f"**{speaker_name}:** {current_text.strip()}\\n\\n"
            current_text = ""

        current_speaker_label = speaker_label
        current_text += " " + seg.text

    # Asegurarse de agregar el √∫ltimo p√°rrafo que qued√≥ en el buffer
    if current_speaker_label != -1:
        speaker_name = f"Hablante {current_speaker_label + 1}"
        transcripcion += f"**{speaker_name}:** {current_text.strip()}\\n\\n"

    return transcripcion.strip()

# --- 4. INTERFAZ DE GRADIO ---

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## üéôÔ∏è Transcriptor con Diarizaci√≥n (Resemblyzer + DBSCAN)")
    gr.Markdown("Graba una conversaci√≥n. El sistema transcribir√° y agrupar√° los segmentos por hablante.")

    with gr.Row():
        mic = gr.Audio(sources=["microphone"], type="filepath", label="üé§ Graba tu audio aqu√≠")

    with gr.Row():
        text_box = gr.Textbox(label="üìù Transcripci√≥n con Hablantes", lines=20, interactive=False)

    mic.change(transcribir_con_diarizacion, inputs=mic, outputs=text_box)

if __name__ == "__main__":
    demo.launch(share=True, debug=True)